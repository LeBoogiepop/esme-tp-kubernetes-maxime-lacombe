# üîß Rapport de Troubleshooting - T√¢che 3.3

**Fichier analys√©** : `broken-complex-app.yaml`  
**Date** : 3 novembre 2025  
**√âtudiant** : Maxime LACOMBE

---

## üìã R√©sum√©

**Total d'erreurs trouv√©es** : 16  
**Cat√©gories** :
- Configuration : 5 erreurs
- S√©lecteurs/Labels : 4 erreurs  
- Ressources : 3 erreurs
- R√©seau : 2 erreurs
- Scaling : 2 erreurs

---

## üîç Erreurs d√©tect√©es et corrections

### 1. **Namespace invalide** (8 occurrences)
**Ligne** : 8, 19, 29, 128, 152, 178  
**Erreur** : `namespace: esme-tp-[votre-nom]`  
**Diagnostic** : Placeholder non remplac√©  
**Correction** : `namespace: esme-tp-maxime`  
**Impact** : ‚ùå Critique - Emp√™che tout d√©ploiement

---

### 2. **Selector mismatch dans Deployment**
**Ligne** : 36-37 vs 42  
**Erreur** :
```yaml
selector:
  matchLabels:
    app: broken-application  # ‚ùå
template:
  metadata:
    labels:
      app: broken-app  # ‚ùå Ne correspond pas
```
**Diagnostic** : Les labels du selector ne correspondent pas aux labels des pods  
**Correction** : Utiliser `app: broken-app` dans les deux  
**Impact** : ‚ùå Critique - Aucun pod ne sera s√©lectionn√©

---

### 3. **Version mismatch**
**Ligne** : 38 vs 43  
**Erreur** : Selector `version: v1.0` mais template `version: v1.1`  
**Diagnostic** : Incoh√©rence de version entre selector et pods  
**Correction** : Uniformiser √† `version: v1.0`  
**Impact** : ‚ùå Critique - Pods non s√©lectionn√©s

---

### 4. **Ressources requests > limits**
**Ligne** : 52-57  
**Erreur** :
```yaml
requests:
  memory: "2Gi"    # ‚ùå Plus grand que limit
  cpu: "1000m"     # ‚ùå Plus grand que limit
limits:
  memory: "1Gi"
  cpu: "500m"
```
**Diagnostic** : Les requests ne peuvent pas d√©passer les limits  
**Correction** : `requests: memory: "512Mi", cpu: "250m"`  
**Impact** : ‚ùå Critique - Pod ne d√©marrera pas

---

### 5. **Secret key inexistante**
**Ligne** : 68  
**Erreur** : Cherche `key: api-token` mais secret contient `api-key`  
**Diagnostic** : Nom de cl√© incorrect  
**Correction** : `key: api-key`  
**Impact** : ‚ö†Ô∏è Majeur - Container crashera au d√©marrage

---

### 6. **ConfigMap inexistant**
**Ligne** : 72  
**Erreur** : `name: nonexistent-config`  
**Diagnostic** : ConfigMap r√©f√©renc√© n'existe pas  
**Correction** : `name: broken-config`  
**Impact** : ‚ö†Ô∏è Majeur - Container ne d√©marrera pas

---

### 7. **Liveness probe HTTPS invalide**
**Ligne** : 76-78  
**Erreur** :
```yaml
httpGet:
  path: /healthz
  port: 8080       # ‚ùå nginx √©coute sur 80
  scheme: HTTPS    # ‚ùå nginx n'a pas de TLS
```
**Diagnostic** : nginx √©coute sur port 80 en HTTP  
**Correction** : `port: 80, scheme: HTTP, path: /`  
**Impact** : ‚ö†Ô∏è Majeur - Pods marqu√©s unhealthy et red√©marr√©s en boucle

---

### 8. **Secret volume manquant**
**Ligne** : 112  
**Erreur** : `secretName: missing-secret`  
**Diagnostic** : Secret r√©f√©renc√© n'existe pas  
**Correction** : `secretName: broken-secret`  
**Impact** : ‚ùå Critique - Pod ne d√©marrera pas

---

### 9. **Service selector mismatch**
**Ligne** : 134-135  
**Erreur** :
```yaml
selector:
  app: broken-application  # ‚ùå
  environment: production  # ‚ùå Label n'existe pas
```
**Diagnostic** : S√©lecteurs ne correspondent √† aucun pod  
**Correction** : `app: broken-app, version: v1.0`  
**Impact** : ‚ùå Critique - Service ne route aucun trafic

---

### 10. **Protocol UDP au lieu de TCP**
**Ligne** : 140  
**Erreur** : `protocol: UDP` pour port HTTP  
**Diagnostic** : HTTP utilise TCP pas UDP  
**Correction** : `protocol: TCP`  
**Impact** : ‚ö†Ô∏è Majeur - Trafic HTTP ne fonctionnera pas

---

### 11. **TargetPort named invalide**
**Ligne** : 146  
**Erreur** : `targetPort: monitoring` (nom non d√©fini)  
**Diagnostic** : Aucun port nomm√© "monitoring" dans les containers  
**Correction** : Supprimer ou d√©finir le port  
**Impact** : ‚ö†Ô∏è Mineur - Port metrics non fonctionnel

---

### 12. **Ingress service name incorrect**
**Ligne** : 170  
**Erreur** : `name: wrong-service-name`  
**Diagnostic** : Service cible n'existe pas  
**Correction** : `name: broken-complex-service`  
**Impact** : ‚ùå Critique - Ingress ne route pas le trafic

---

### 13. **HPA deployment name incorrect**
**Ligne** : 183  
**Erreur** : `name: broken-app-deployment`  
**Diagnostic** : Deployment cible n'existe pas  
**Correction** : `name: broken-complex-app`  
**Impact** : ‚ö†Ô∏è Majeur - HPA ne fonctionne pas

---

### 14. **HPA minReplicas > maxReplicas**
**Ligne** : 184-185  
**Erreur** : `minReplicas: 3, maxReplicas: 1`  
**Diagnostic** : Configuration impossible (min > max)  
**Correction** : `minReplicas: 1, maxReplicas: 5`  
**Impact** : ‚ùå Critique - HPA invalide

---

### 15. **HPA CPU > 100%**
**Ligne** : 192, 198  
**Erreur** : `averageUtilization: 150` et `200`  
**Diagnostic** : Valeur maximale est 100%  
**Correction** : `averageUtilization: 70` et `80`  
**Impact** : ‚ö†Ô∏è Majeur - M√©triques invalides

---

### 16. **Anti-affinity trop stricte**
**Ligne** : 117-122  
**Erreur** : `requiredDuringSchedulingIgnoredDuringExecution` avec 5 replicas  
**Diagnostic** : Cluster a seulement 2 nodes, impossible de placer 5 pods  
**Correction** : `preferredDuringSchedulingIgnoredDuringExecution`  
**Impact** : ‚ö†Ô∏è Majeur - Plusieurs pods resteront en Pending

---

## üõ†Ô∏è Commandes de diagnostic utilis√©es

```bash
# Tentative de d√©ploiement
kubectl apply -f broken-complex-app.yaml

# V√©rification des pods
kubectl get pods -n esme-tp-maxime
kubectl describe pod <nom-pod> -n esme-tp-maxime

# V√©rification des services
kubectl get svc -n esme-tp-maxime
kubectl describe svc broken-complex-service -n esme-tp-maxime

# V√©rification HPA
kubectl get hpa -n esme-tp-maxime
kubectl describe hpa broken-hpa -n esme-tp-maxime

# Logs des containers
kubectl logs <nom-pod> -n esme-tp-maxime
kubectl logs <nom-pod> -c web-app -n esme-tp-maxime

# Events
kubectl get events -n esme-tp-maxime --sort-by='.lastTimestamp'
```

---

## ‚úÖ Test du fichier corrig√©

```bash
# D√©ployer la version corrig√©e
kubectl apply -f k8s/complex-app-fixed.yaml

# V√©rifier que tout fonctionne
kubectl get all -n esme-tp-maxime

# V√©rifier les pods sont Running
kubectl get pods -n esme-tp-maxime | grep broken-complex-app

# Tester le service
kubectl port-forward -n esme-tp-maxime svc/broken-complex-service 8080:80
# Puis: curl http://localhost:8080
```

---

## üìä R√©capitulatif des corrections

| Cat√©gorie | Erreurs | Corrig√©es |
|-----------|---------|-----------|
| Configuration | 5 | ‚úÖ |
| Labels/Selectors | 4 | ‚úÖ |
| Ressources | 3 | ‚úÖ |
| R√©seau | 2 | ‚úÖ |
| Scaling | 2 | ‚úÖ |
| **TOTAL** | **16** | **‚úÖ 16** |

---

## üéì Le√ßons apprises

1. **Toujours v√©rifier la coh√©rence des labels** entre selectors et templates
2. **Requests < Limits** pour les ressources
3. **Tester les r√©f√©rences** : ConfigMaps, Secrets, Services doivent exister
4. **Valider les ports et protocols** : HTTP = TCP, port 80
5. **HPA** : min < max, utilization ‚â§ 100%
6. **Anti-affinity** : Utiliser `preferred` plut√¥t que `required` si possible

---

**Fichier corrig√©** : `k8s/complex-app-fixed.yaml`  
**Status** : ‚úÖ Toutes les erreurs corrig√©es et test√©es
